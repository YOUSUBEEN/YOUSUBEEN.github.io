---
layout: single
title:  "수빈좌 첫 분석 성공"
categories: test
tags: [python, blog, '축배를 들어라']
author_profile: false
toc: true
toc_sticky: true
toc_label: 목차
---

```python
import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
```


```python
data = pd.read_csv('test_1.csv', encoding = 'cp949')
data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>jb</th>
      <th>ahr</th>
      <th>sp</th>
      <th>ap</th>
      <th>grdp</th>
      <th>gj</th>
      <th>go</th>
      <th>cgo</th>
      <th>sup</th>
      <th>pits</th>
      <th>pses</th>
      <th>hos</th>
      <th>ppsp</th>
      <th>pls</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>37</td>
      <td>5</td>
      <td>90</td>
      <td>-33</td>
      <td>100</td>
      <td>56</td>
      <td>55</td>
      <td>58</td>
      <td>2</td>
      <td>19</td>
      <td>31</td>
      <td>3</td>
      <td>90</td>
      <td>-22</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>41</td>
      <td>29</td>
      <td>8</td>
      <td>27</td>
      <td>99</td>
      <td>67</td>
      <td>66</td>
      <td>68</td>
      <td>2</td>
      <td>30</td>
      <td>26</td>
      <td>2</td>
      <td>94</td>
      <td>78</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>42</td>
      <td>37</td>
      <td>84</td>
      <td>38</td>
      <td>100</td>
      <td>66</td>
      <td>65</td>
      <td>70</td>
      <td>2</td>
      <td>75</td>
      <td>60</td>
      <td>12</td>
      <td>56</td>
      <td>51</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>43</td>
      <td>39</td>
      <td>78</td>
      <td>19</td>
      <td>100</td>
      <td>65</td>
      <td>65</td>
      <td>69</td>
      <td>1</td>
      <td>40</td>
      <td>69</td>
      <td>3</td>
      <td>89</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>40</td>
      <td>35</td>
      <td>42</td>
      <td>28</td>
      <td>99</td>
      <td>66</td>
      <td>65</td>
      <td>72</td>
      <td>2</td>
      <td>26</td>
      <td>39</td>
      <td>1</td>
      <td>96</td>
      <td>66</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>107</th>
      <td>1</td>
      <td>47</td>
      <td>41</td>
      <td>88</td>
      <td>53</td>
      <td>100</td>
      <td>79</td>
      <td>79</td>
      <td>83</td>
      <td>1</td>
      <td>58</td>
      <td>70</td>
      <td>8</td>
      <td>64</td>
      <td>73</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0</td>
      <td>36</td>
      <td>21</td>
      <td>-321</td>
      <td>-26</td>
      <td>92</td>
      <td>64</td>
      <td>62</td>
      <td>67</td>
      <td>3</td>
      <td>3</td>
      <td>9</td>
      <td>6</td>
      <td>74</td>
      <td>28</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0</td>
      <td>45</td>
      <td>27</td>
      <td>-21</td>
      <td>15</td>
      <td>98</td>
      <td>65</td>
      <td>64</td>
      <td>68</td>
      <td>1</td>
      <td>10</td>
      <td>7</td>
      <td>8</td>
      <td>67</td>
      <td>33</td>
    </tr>
    <tr>
      <th>110</th>
      <td>1</td>
      <td>47</td>
      <td>29</td>
      <td>62</td>
      <td>25</td>
      <td>99</td>
      <td>72</td>
      <td>72</td>
      <td>75</td>
      <td>1</td>
      <td>25</td>
      <td>71</td>
      <td>11</td>
      <td>51</td>
      <td>66</td>
    </tr>
    <tr>
      <th>111</th>
      <td>1</td>
      <td>44</td>
      <td>28</td>
      <td>50</td>
      <td>15</td>
      <td>99</td>
      <td>67</td>
      <td>66</td>
      <td>70</td>
      <td>2</td>
      <td>10</td>
      <td>54</td>
      <td>1</td>
      <td>100</td>
      <td>45</td>
    </tr>
  </tbody>
</table>
<p>112 rows × 15 columns</p>
</div>




```python
data.isna().sum()
```




    target    0
    jb        0
    ahr       0
    sp        0
    ap        0
    grdp      0
    gj        0
    go        0
    cgo       0
    sup       0
    pits      0
    pses      0
    hos       0
    ppsp      0
    pls       0
    dtype: int64




```python
# NaN 값을 중간값으로 대체
data = data.fillna(data.median())
data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>jb</th>
      <th>ahr</th>
      <th>sp</th>
      <th>ap</th>
      <th>grdp</th>
      <th>gj</th>
      <th>go</th>
      <th>cgo</th>
      <th>sup</th>
      <th>pits</th>
      <th>pses</th>
      <th>hos</th>
      <th>ppsp</th>
      <th>pls</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>37</td>
      <td>5</td>
      <td>90</td>
      <td>-33</td>
      <td>100</td>
      <td>56</td>
      <td>55</td>
      <td>58</td>
      <td>2</td>
      <td>19</td>
      <td>31</td>
      <td>3</td>
      <td>90</td>
      <td>-22</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>41</td>
      <td>29</td>
      <td>8</td>
      <td>27</td>
      <td>99</td>
      <td>67</td>
      <td>66</td>
      <td>68</td>
      <td>2</td>
      <td>30</td>
      <td>26</td>
      <td>2</td>
      <td>94</td>
      <td>78</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>42</td>
      <td>37</td>
      <td>84</td>
      <td>38</td>
      <td>100</td>
      <td>66</td>
      <td>65</td>
      <td>70</td>
      <td>2</td>
      <td>75</td>
      <td>60</td>
      <td>12</td>
      <td>56</td>
      <td>51</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>43</td>
      <td>39</td>
      <td>78</td>
      <td>19</td>
      <td>100</td>
      <td>65</td>
      <td>65</td>
      <td>69</td>
      <td>1</td>
      <td>40</td>
      <td>69</td>
      <td>3</td>
      <td>89</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>40</td>
      <td>35</td>
      <td>42</td>
      <td>28</td>
      <td>99</td>
      <td>66</td>
      <td>65</td>
      <td>72</td>
      <td>2</td>
      <td>26</td>
      <td>39</td>
      <td>1</td>
      <td>96</td>
      <td>66</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>107</th>
      <td>1</td>
      <td>47</td>
      <td>41</td>
      <td>88</td>
      <td>53</td>
      <td>100</td>
      <td>79</td>
      <td>79</td>
      <td>83</td>
      <td>1</td>
      <td>58</td>
      <td>70</td>
      <td>8</td>
      <td>64</td>
      <td>73</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0</td>
      <td>36</td>
      <td>21</td>
      <td>-321</td>
      <td>-26</td>
      <td>92</td>
      <td>64</td>
      <td>62</td>
      <td>67</td>
      <td>3</td>
      <td>3</td>
      <td>9</td>
      <td>6</td>
      <td>74</td>
      <td>28</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0</td>
      <td>45</td>
      <td>27</td>
      <td>-21</td>
      <td>15</td>
      <td>98</td>
      <td>65</td>
      <td>64</td>
      <td>68</td>
      <td>1</td>
      <td>10</td>
      <td>7</td>
      <td>8</td>
      <td>67</td>
      <td>33</td>
    </tr>
    <tr>
      <th>110</th>
      <td>1</td>
      <td>47</td>
      <td>29</td>
      <td>62</td>
      <td>25</td>
      <td>99</td>
      <td>72</td>
      <td>72</td>
      <td>75</td>
      <td>1</td>
      <td>25</td>
      <td>71</td>
      <td>11</td>
      <td>51</td>
      <td>66</td>
    </tr>
    <tr>
      <th>111</th>
      <td>1</td>
      <td>44</td>
      <td>28</td>
      <td>50</td>
      <td>15</td>
      <td>99</td>
      <td>67</td>
      <td>66</td>
      <td>70</td>
      <td>2</td>
      <td>10</td>
      <td>54</td>
      <td>1</td>
      <td>100</td>
      <td>45</td>
    </tr>
  </tbody>
</table>
<p>112 rows × 15 columns</p>
</div>




```python
# 데이터프레임에서 상관 계수 계산
import matplotlib.pyplot as plt
plt.rcParams['font.family']='sans-serif'
plt.rcParams['font.sans-serif']= ['Arial']
corr_matrix = data.corr()

# 상관 관계 히트맵 시각화
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
```




    <AxesSubplot:>




    
![png](output_4_1.png)
    



```python
pp = data.drop(['target', 'go','cgo','hos'], axis=1)
pp_matrix = pp.corr()
sns.heatmap(pp_matrix, annot=True, cmap='coolwarm')
```




    <AxesSubplot:>




    
![png](output_5_1.png)
    



```python
corr_matrix
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>jb</th>
      <th>ahr</th>
      <th>sp</th>
      <th>ap</th>
      <th>grdp</th>
      <th>gj</th>
      <th>go</th>
      <th>cgo</th>
      <th>sup</th>
      <th>pits</th>
      <th>pses</th>
      <th>hos</th>
      <th>ppsp</th>
      <th>pls</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>target</th>
      <td>1.000000</td>
      <td>0.710901</td>
      <td>0.523316</td>
      <td>0.691410</td>
      <td>0.709547</td>
      <td>0.742554</td>
      <td>0.301010</td>
      <td>0.354712</td>
      <td>0.399428</td>
      <td>-0.404948</td>
      <td>0.550663</td>
      <td>0.652526</td>
      <td>-0.319458</td>
      <td>0.331347</td>
      <td>0.367061</td>
    </tr>
    <tr>
      <th>jb</th>
      <td>0.710901</td>
      <td>1.000000</td>
      <td>0.586481</td>
      <td>0.614582</td>
      <td>0.738498</td>
      <td>0.730451</td>
      <td>0.460305</td>
      <td>0.514157</td>
      <td>0.596274</td>
      <td>-0.505116</td>
      <td>0.577865</td>
      <td>0.630632</td>
      <td>-0.305830</td>
      <td>0.299083</td>
      <td>0.501328</td>
    </tr>
    <tr>
      <th>ahr</th>
      <td>0.523316</td>
      <td>0.586481</td>
      <td>1.000000</td>
      <td>0.414901</td>
      <td>0.567899</td>
      <td>0.434607</td>
      <td>0.330003</td>
      <td>0.401192</td>
      <td>0.487539</td>
      <td>-0.401497</td>
      <td>0.485217</td>
      <td>0.588570</td>
      <td>-0.161005</td>
      <td>0.155483</td>
      <td>0.470853</td>
    </tr>
    <tr>
      <th>sp</th>
      <td>0.691410</td>
      <td>0.614582</td>
      <td>0.414901</td>
      <td>1.000000</td>
      <td>0.501827</td>
      <td>0.724498</td>
      <td>0.226792</td>
      <td>0.281637</td>
      <td>0.300343</td>
      <td>-0.368310</td>
      <td>0.542101</td>
      <td>0.623820</td>
      <td>-0.524391</td>
      <td>0.526361</td>
      <td>0.231825</td>
    </tr>
    <tr>
      <th>ap</th>
      <td>0.709547</td>
      <td>0.738498</td>
      <td>0.567899</td>
      <td>0.501827</td>
      <td>1.000000</td>
      <td>0.613356</td>
      <td>0.338877</td>
      <td>0.419120</td>
      <td>0.456186</td>
      <td>-0.359346</td>
      <td>0.503491</td>
      <td>0.650484</td>
      <td>-0.129319</td>
      <td>0.133878</td>
      <td>0.395641</td>
    </tr>
    <tr>
      <th>grdp</th>
      <td>0.742554</td>
      <td>0.730451</td>
      <td>0.434607</td>
      <td>0.724498</td>
      <td>0.613356</td>
      <td>1.000000</td>
      <td>0.229804</td>
      <td>0.283470</td>
      <td>0.295935</td>
      <td>-0.479557</td>
      <td>0.600537</td>
      <td>0.689247</td>
      <td>-0.354832</td>
      <td>0.362098</td>
      <td>0.357169</td>
    </tr>
    <tr>
      <th>gj</th>
      <td>0.301010</td>
      <td>0.460305</td>
      <td>0.330003</td>
      <td>0.226792</td>
      <td>0.338877</td>
      <td>0.229804</td>
      <td>1.000000</td>
      <td>0.978911</td>
      <td>0.906962</td>
      <td>-0.324963</td>
      <td>0.326585</td>
      <td>0.482427</td>
      <td>-0.019262</td>
      <td>0.007542</td>
      <td>0.443905</td>
    </tr>
    <tr>
      <th>go</th>
      <td>0.354712</td>
      <td>0.514157</td>
      <td>0.401192</td>
      <td>0.281637</td>
      <td>0.419120</td>
      <td>0.283470</td>
      <td>0.978911</td>
      <td>1.000000</td>
      <td>0.931727</td>
      <td>-0.455225</td>
      <td>0.388694</td>
      <td>0.540120</td>
      <td>-0.029391</td>
      <td>0.017406</td>
      <td>0.490275</td>
    </tr>
    <tr>
      <th>cgo</th>
      <td>0.399428</td>
      <td>0.596274</td>
      <td>0.487539</td>
      <td>0.300343</td>
      <td>0.456186</td>
      <td>0.295935</td>
      <td>0.906962</td>
      <td>0.931727</td>
      <td>1.000000</td>
      <td>-0.499705</td>
      <td>0.428964</td>
      <td>0.536487</td>
      <td>-0.015340</td>
      <td>0.003013</td>
      <td>0.512691</td>
    </tr>
    <tr>
      <th>sup</th>
      <td>-0.404948</td>
      <td>-0.505116</td>
      <td>-0.401497</td>
      <td>-0.368310</td>
      <td>-0.359346</td>
      <td>-0.479557</td>
      <td>-0.324963</td>
      <td>-0.455225</td>
      <td>-0.499705</td>
      <td>1.000000</td>
      <td>-0.415412</td>
      <td>-0.405903</td>
      <td>0.156018</td>
      <td>-0.159277</td>
      <td>-0.332467</td>
    </tr>
    <tr>
      <th>pits</th>
      <td>0.550663</td>
      <td>0.577865</td>
      <td>0.485217</td>
      <td>0.542101</td>
      <td>0.503491</td>
      <td>0.600537</td>
      <td>0.326585</td>
      <td>0.388694</td>
      <td>0.428964</td>
      <td>-0.415412</td>
      <td>1.000000</td>
      <td>0.592680</td>
      <td>-0.152051</td>
      <td>0.146135</td>
      <td>0.514990</td>
    </tr>
    <tr>
      <th>pses</th>
      <td>0.652526</td>
      <td>0.630632</td>
      <td>0.588570</td>
      <td>0.623820</td>
      <td>0.650484</td>
      <td>0.689247</td>
      <td>0.482427</td>
      <td>0.540120</td>
      <td>0.536487</td>
      <td>-0.405903</td>
      <td>0.592680</td>
      <td>1.000000</td>
      <td>-0.244639</td>
      <td>0.244091</td>
      <td>0.422659</td>
    </tr>
    <tr>
      <th>hos</th>
      <td>-0.319458</td>
      <td>-0.305830</td>
      <td>-0.161005</td>
      <td>-0.524391</td>
      <td>-0.129319</td>
      <td>-0.354832</td>
      <td>-0.019262</td>
      <td>-0.029391</td>
      <td>-0.015340</td>
      <td>0.156018</td>
      <td>-0.152051</td>
      <td>-0.244639</td>
      <td>1.000000</td>
      <td>-0.996344</td>
      <td>-0.033704</td>
    </tr>
    <tr>
      <th>ppsp</th>
      <td>0.331347</td>
      <td>0.299083</td>
      <td>0.155483</td>
      <td>0.526361</td>
      <td>0.133878</td>
      <td>0.362098</td>
      <td>0.007542</td>
      <td>0.017406</td>
      <td>0.003013</td>
      <td>-0.159277</td>
      <td>0.146135</td>
      <td>0.244091</td>
      <td>-0.996344</td>
      <td>1.000000</td>
      <td>0.041013</td>
    </tr>
    <tr>
      <th>pls</th>
      <td>0.367061</td>
      <td>0.501328</td>
      <td>0.470853</td>
      <td>0.231825</td>
      <td>0.395641</td>
      <td>0.357169</td>
      <td>0.443905</td>
      <td>0.490275</td>
      <td>0.512691</td>
      <td>-0.332467</td>
      <td>0.514990</td>
      <td>0.422659</td>
      <td>-0.033704</td>
      <td>0.041013</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 전체 데이터셋을 X로 준비
X = data.drop(['target','go','hos','jb','grdp','cgo','pses','gj'], axis=1)  # 종속 변수를 제외한 독립 변수들

# 각 변수의 VIF 값을 계산
vif = pd.DataFrame()
vif["Feature"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# 결과 출력
print(vif)
```

      Feature       VIF
    0     ahr  8.215053
    1      sp  2.210776
    2      ap  1.963488
    3     sup  3.818305
    4    pits  5.672687
    5    ppsp  4.306160
    6     pls  7.907070
    

'target', 'ap', 'sp', 'grdp', 'gj', 'go', 'cgo', 'sup',
       'pits', 'pses', 'gra', 'hos', 'br'


```python
# target 열을 제외한 열 선택
data_vif = data[['target','ap','ahr','pits','pls','ppsp']]

data_vif_normalize = data_vif.columns[data_vif.columns != 'target']

# Min-Max 정규화 함수
normalize = lambda x: (x - x.min()) / (x.max() - x.min())

#값이 0인 경우 최솟값으로 대체하는 함수
#replace_zero_with_min = lambda x:x.replace(0, x.min())

# # 선택한 열에 대해 정규화 수행
data_vif[data_vif_normalize] = data_vif[data_vif_normalize].apply(normalize)
data_vif
# data_vif_copy = data_vif.copy()
# data_vif_copy[data_vif_normalize] = data_vif[data_vif_normalize].apply(normalize)
# data_vif

```

    C:\Users\yousb\AppData\Local\Temp\ipykernel_23908\1266228478.py:13: SettingWithCopyWarning: 
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead
    
    See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
      data_vif[data_vif_normalize] = data_vif[data_vif_normalize].apply(normalize)
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>ap</th>
      <th>ahr</th>
      <th>pits</th>
      <th>pls</th>
      <th>ppsp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0.571429</td>
      <td>0.066667</td>
      <td>0.181818</td>
      <td>0.290541</td>
      <td>0.968051</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.857143</td>
      <td>0.600000</td>
      <td>0.292929</td>
      <td>0.966216</td>
      <td>0.980831</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0.909524</td>
      <td>0.777778</td>
      <td>0.747475</td>
      <td>0.783784</td>
      <td>0.859425</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0.819048</td>
      <td>0.822222</td>
      <td>0.393939</td>
      <td>0.493243</td>
      <td>0.964856</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0.861905</td>
      <td>0.733333</td>
      <td>0.252525</td>
      <td>0.885135</td>
      <td>0.987220</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>107</th>
      <td>1</td>
      <td>0.980952</td>
      <td>0.866667</td>
      <td>0.575758</td>
      <td>0.932432</td>
      <td>0.884984</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0</td>
      <td>0.604762</td>
      <td>0.422222</td>
      <td>0.020202</td>
      <td>0.628378</td>
      <td>0.916933</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0</td>
      <td>0.800000</td>
      <td>0.555556</td>
      <td>0.090909</td>
      <td>0.662162</td>
      <td>0.894569</td>
    </tr>
    <tr>
      <th>110</th>
      <td>1</td>
      <td>0.847619</td>
      <td>0.600000</td>
      <td>0.242424</td>
      <td>0.885135</td>
      <td>0.843450</td>
    </tr>
    <tr>
      <th>111</th>
      <td>1</td>
      <td>0.800000</td>
      <td>0.577778</td>
      <td>0.090909</td>
      <td>0.743243</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>112 rows × 6 columns</p>
</div>




```python
# 로지스틱 회귀분석을 위한 독립변수와 종속변수 분리
X = data_vif.drop('target', axis=1)
y = data_vif['target']

# 상수(intercept) 열 추가
X = sm.add_constant(X)

# 로지스틱 회귀모델 학습
logit_model = sm.Logit(y, X)
result = logit_model.fit(method='newton')

# 회귀분석 결과 요약
summary = result.summary()
print(summary)

# 후진소거법으로 제거 완료
```

    Optimization terminated successfully.
             Current function value: 0.173405
             Iterations 9
                               Logit Regression Results                           
    ==============================================================================
    Dep. Variable:                 target   No. Observations:                  112
    Model:                          Logit   Df Residuals:                      106
    Method:                           MLE   Df Model:                            5
    Date:                Mon, 05 Jun 2023   Pseudo R-squ.:                  0.6968
    Time:                        21:41:53   Log-Likelihood:                -19.421
    converged:                       True   LL-Null:                       -64.057
    Covariance Type:            nonrobust   LLR p-value:                 9.563e-18
    ==============================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const        -23.5489      8.190     -2.875      0.004     -39.600      -7.497
    ap            13.2477      4.952      2.675      0.007       3.542      22.953
    ahr            3.2129      2.492      1.289      0.197      -1.671       8.097
    pits          12.7556      4.764      2.677      0.007       3.418      22.093
    pls           -2.6405      2.641     -1.000      0.317      -7.818       2.537
    ppsp          13.8047      7.022      1.966      0.049       0.042      27.567
    ==============================================================================
    

로지스틱 회귀모델에서 R-squ는 종속변수의 변동성을 독립 변수로 설명하는 비율을 나타내는 지표가 아님.
주로 선형 회귀모델에서 사용되며 로지스틱 회귀에서는 적절한 평가 지표가 아님.


```python
# 다중공선성 한번더 확인 
from patsy import dmatrices

y, X = dmatrices('target~pits+ap+ppsp+ahr+pls', data_vif, return_type='dataframe')

vif =pd.DataFrame()
vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif['feature'] =X.columns
vif
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF Factor</th>
      <th>feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>82.380393</td>
      <td>Intercept</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.654311</td>
      <td>pits</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.650231</td>
      <td>ap</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.038594</td>
      <td>ppsp</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.716702</td>
      <td>ahr</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1.505699</td>
      <td>pls</td>
    </tr>
  </tbody>
</table>
</div>




```python
import numpy as np
np.exp(result.params)
```




    const    5.927217e-11
    ap       5.667846e+05
    ahr      2.485132e+01
    pits     3.464720e+05
    pls      7.132368e-02
    ppsp     9.892177e+05
    dtype: float64



- 주택가격의 격차가 1만원 오를 때 도시 양극화가 될 확률이 5.92e-11배 증가한다.
- 어린이집 및 유치원 서비스권역 내 영유아인구 비율 차이가 1% 오를 때 도시 양극화가 될 확률이 24.8배 증가한다. 
- 공공체육시설 서비스권역 내 인구비율 차이가 1% 오를 때 9.89e+05배 증가한다.
- 1인당 소비가격 격차는 양극화에 큰 영향을 미쳐서 변수로 적용 시 완전한 분리가 일어난다. 
    + 1인당 소비가격격차가 도시 양극화에 큰 영향을 미친다. 


```python
from sklearn.metrics import accuracy_score
from sklearn.metrics import log_loss

#예측값 계산
y_pred_proba = result.predict(X)
y_pred = (y_pred_proba > 0.5).astype(int)

#로그 손실 계산
loss = log_loss(y, y_pred_proba)

#정확도 계산
accuracy = accuracy_score(y, y_pred)
```


```python
print("로그손실:",loss)
print("정확도:", accuracy)
```

    로그손실: 0.44327068342456705
    정확도: 0.8660714285714286
    


```python
from nbconvert import MarkdownExporter
import nbformat

def convert_notebook_to_markdown(notebook_path, output_path):
    # 노트북 파일 로드
    with open(notebook_path, 'r', encoding='utf-8') as f:
        nb = nbformat.read(f, as_version=4)
    
    # Markdown 변환기 생성
    md_exporter = MarkdownExporter()
    
    # Markdown으로 변환
    (body, resources) = md_exporter.from_notebook_node(nb)
    
    # Markdown 파일 저장
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(body)

# 변환할 노트북 파일 경로와 저장할 Markdown 파일 경로 지정
notebook_path = 'path/to/your_notebook.ipynb'
output_path = 'path/to/output.md'

# 노트북을 Markdown으로 변환
convert_notebook_to_markdown(notebook_path, output_path)
```
